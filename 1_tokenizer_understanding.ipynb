{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc0a0eb",
   "metadata": {},
   "source": [
    "### WHITE SPACE TOKENIZERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2414ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total numbers of charecters -  1607894\n",
      "\n",
      " A Game Of Thrones \n",
      "Book One of A Song of Ice and Fire \n",
      "By George R. R. Martin \n",
      "PROLOGUE \n",
      "\"We should start back,\" Gared u\n"
     ]
    }
   ],
   "source": [
    "# reading the book\n",
    "with open(r\"Game_of_thrones_books/001ssb.txt\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "\n",
    "print(\"total numbers of charecters - \",len(raw_text))\n",
    "print('\\n',raw_text[:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd4e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'i', 'am', 'ismail', 'khan', ',', 'i', 'love', 'NLP', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "#Checking the whitespace tokeniser\n",
    "import re\n",
    "text=\"\"\"Hi, i am ismail khan, i love NLP!!\"\"\"\n",
    "result=re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item for item in result if item.strip()] ## removing the spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a26a98df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Game', 'Of', 'Thrones', 'Book', 'One', 'of', 'A', 'Song', 'of', 'Ice', 'and', 'Fire', 'By', 'George', 'R', '.', 'R', '.', 'Martin', 'PROLOGUE', '\"', 'We', 'should', 'start', 'back', ',', '\"', 'Gared', 'urged']\n",
      "\n",
      " 367561\n"
     ]
    }
   ],
   "source": [
    "# processing the GOT book\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "print('\\n', len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b66fcadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Vocabulary - creating tokens id\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "vocab_size = len(all_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5f32676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab): # here vocab is text\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} # reverse the mapping of token-id and make id-token fro decoder to fetch words for id while predicting \n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a88e9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 801, 1900, 8961, 609, 2262, 8961, 1471, 2922, 1195]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"Book One of A Song of Ice and Fire\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c7eb995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" Book One of A Song of Ice and Fire'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e11b424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13724]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"ismail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100832ff",
   "metadata": {},
   "source": [
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7351818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1401,\n",
       " 7,\n",
       " 5297,\n",
       " 13706,\n",
       " 8063,\n",
       " 12250,\n",
       " 608,\n",
       " 13723,\n",
       " 1485,\n",
       " 12325,\n",
       " 12051,\n",
       " 13724,\n",
       " 8961,\n",
       " 12325,\n",
       " 9158,\n",
       " 24]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e28104f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit <|unk|> of the palace.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87451dc",
   "metadata": {},
   "source": [
    "### BYTE PAIR TOKENIZERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec5c4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "956d9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2a0c695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6242, 3861, 42, 2885, 6242, 3861]\n"
     ]
    }
   ],
   "source": [
    "text = ( \"ABRAKADABRA\")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd517f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB\n",
      "RA\n",
      "K\n",
      "AD\n",
      "AB\n",
      "RA\n"
     ]
    }
   ],
   "source": [
    "for x in integers:\n",
    "    print(tokenizer.decode([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff4e1b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABRAKADABRA\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad27e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
